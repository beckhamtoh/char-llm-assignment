{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/beckhamtoh/char-llm-assignment.git\n",
        "%cd char-llm-assignment\n",
        "!ls"
      ],
      "metadata": {
        "id": "LNS1ztqUUrw7",
        "outputId": "fd2ffd8e-22be-4ba6-c12d-6ef75a6b79b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LNS1ztqUUrw7",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'char-llm-assignment'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 18 (delta 1), reused 11 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (18/18), 30.17 MiB | 16.77 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/char-llm-assignment/char-llm-assignment/char-llm-assignment/char-llm-assignment\n",
            "data  models  README.md  transformer.ipynb  util\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e2def0f4",
      "metadata": {
        "id": "e2def0f4"
      },
      "outputs": [],
      "source": [
        "# Enable autoreload of local Python modules (e.g., models)\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "# manual reload for local modules\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "d5001b05",
      "metadata": {
        "id": "d5001b05"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import time\n",
        "\n",
        "# local imports\n",
        "import models.models as models\n",
        "import util.generation as generation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LOGGING: small, no-deps experiment logger ====\n",
        "import os, csv, json, time, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "class ExperimentLogger:\n",
        "    def __init__(self, root=\"runs\", run_tag=\"run\"):\n",
        "        Path(root).mkdir(parents=True, exist_ok=True)\n",
        "        ts = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        rid = f\"{ts}_{run_tag}_{hashlib.md5(str(time.time()).encode()).hexdigest()[:4]}\"\n",
        "        self.root = Path(root); self.run_id = rid; self.run_dir = self.root / rid\n",
        "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.master = self.root / \"experiments.csv\"\n",
        "        self.eval_csv = self.run_dir / \"eval_log.csv\"\n",
        "        self.samples_txt = self.run_dir / \"samples.txt\"\n",
        "        self.t0 = time.time()\n",
        "        self.best = None; self.best_row = None\n",
        "\n",
        "        # init header files\n",
        "        if not self.master.exists():\n",
        "            with open(self.master, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    \"run_id\",\"run_tag\",\"seed\",\"B\",\"T\",\"steps\",\"tokens_per_step\",\"total_tokens\",\n",
        "                    \"vocab_size\",\"d_model\",\"n_layers\",\"n_heads\",\"max_len\",\"lr\",\"optimizer\",\n",
        "                    \"params\",\"wall_time_s\",\n",
        "                    \"val_loss_best\",\"val_bpc_best\",\"val_acc_best\",\"val_acc_last_best\",\"step_best\",\n",
        "                    \"val_loss_final\",\"val_bpc_final\",\"val_acc_final\",\"val_acc_last_final\",\"step_final\"\n",
        "                ])\n",
        "        with open(self.eval_csv, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([\"step\",\"elapsed_s\",\"val_loss\",\"val_bpc\",\"val_acc\",\"val_acc_last\",\"tokens_seen\"])\n",
        "\n",
        "    def log_config(self, cfg:dict):\n",
        "        with open(self.run_dir / \"config.json\", \"w\") as f:\n",
        "            json.dump(cfg, f, indent=2)\n",
        "        self.cfg = cfg\n",
        "        self.tokens_per_step = int(cfg[\"B\"]) * int(cfg[\"T\"])\n",
        "\n",
        "    def log_eval(self, step:int, val_loss:float, val_acc:float, val_acc_last:float):\n",
        "        elapsed = time.time() - self.t0\n",
        "        val_bpc = float(val_loss) / float(jnp.log(2.0))\n",
        "        tokens_seen = step * self.tokens_per_step\n",
        "        with open(self.eval_csv, \"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([step, f\"{elapsed:.2f}\", float(val_loss), val_bpc, float(val_acc), float(val_acc_last), tokens_seen])\n",
        "\n",
        "        crit = (val_bpc, -val_acc_last)  # lower bpc, higher last-acc is better\n",
        "        if (self.best is None) or (crit < self.best):\n",
        "            self.best = crit\n",
        "            self.best_row = dict(step_best=step,\n",
        "                                 val_loss_best=float(val_loss),\n",
        "                                 val_bpc_best=val_bpc,\n",
        "                                 val_acc_best=float(val_acc),\n",
        "                                 val_acc_last_best=float(val_acc_last))\n",
        "\n",
        "    def save_sample(self, prompt:str, generated:str):\n",
        "        with open(self.samples_txt, \"a\") as f:\n",
        "            f.write(\"=== SAMPLE ===\\nPrompt:\\n\" + prompt + \"\\n\\nGenerated:\\n\" + generated + \"\\n\\n\")\n",
        "\n",
        "    def finish(self, params_count:int, final_row:dict):\n",
        "        wall = time.time() - self.t0\n",
        "        cfg = self.cfg\n",
        "        total_tokens = int(cfg[\"steps\"]) * self.tokens_per_step\n",
        "        row = [\n",
        "            self.run_id, cfg.get(\"run_tag\",\"\"), cfg.get(\"seed\",\"\"), cfg[\"B\"], cfg[\"T\"], cfg[\"steps\"],\n",
        "            self.tokens_per_step, total_tokens,\n",
        "            cfg.get(\"vocab_size\",\"\"), cfg.get(\"d_model\",\"\"), cfg.get(\"n_layers\",\"\"), cfg.get(\"n_heads\",\"\"),\n",
        "            cfg.get(\"max_len\",\"\"), cfg.get(\"lr\",\"\"), cfg.get(\"optimizer\",\"\"),\n",
        "            params_count, f\"{wall:.2f}\",\n",
        "            # best snapshot\n",
        "            self.best_row.get(\"val_loss_best\",\"\") if self.best_row else \"\",\n",
        "            self.best_row.get(\"val_bpc_best\",\"\") if self.best_row else \"\",\n",
        "            self.best_row.get(\"val_acc_best\",\"\") if self.best_row else \"\",\n",
        "            self.best_row.get(\"val_acc_last_best\",\"\") if self.best_row else \"\",\n",
        "            self.best_row.get(\"step_best\",\"\") if self.best_row else \"\",\n",
        "            # final snapshot\n",
        "            final_row[\"val_loss_final\"], final_row[\"val_bpc_final\"],\n",
        "            final_row[\"val_acc_final\"], final_row[\"val_acc_last_final\"], final_row[\"step_final\"],\n",
        "        ]\n",
        "        with open(self.master, \"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(row)\n",
        "        print(f\"[LOG] Wrote master row → {self.master}\")\n",
        "        print(f\"[LOG] Artifacts in → {self.run_dir}\")\n",
        "# ==== /logger ====\n"
      ],
      "metadata": {
        "id": "iP0XpQqB6y1X"
      },
      "id": "iP0XpQqB6y1X",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "906db4f2",
      "metadata": {
        "id": "906db4f2"
      },
      "outputs": [],
      "source": [
        "# initialize the jax random key\n",
        "key = jax.random.key(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762a1c8f",
      "metadata": {
        "id": "762a1c8f"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "86825275",
      "metadata": {
        "id": "86825275",
        "outputId": "d8325b61-be67-4403-dd61-87efb7a7646a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training text: 90_000_000 characters\n",
            "Length of test text: 5_000_000 characters\n"
          ]
        }
      ],
      "source": [
        "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
        "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
        "    train_text = f.read()\n",
        "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
        "    test_text = f.read()\n",
        "\n",
        "# print the length of the training text and test text\n",
        "print(f\"Length of training text: {len(train_text):_} characters\")\n",
        "print(f\"Length of test text: {len(test_text):_} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfdca63",
      "metadata": {
        "id": "7bfdca63"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "34b1eafe",
      "metadata": {
        "id": "34b1eafe"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary (lowercase + space + a few punctuations)\n",
        "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
        "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
        "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
        "\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to array of integers\"\"\"\n",
        "    ids = [char_to_int[c] for c in s]\n",
        "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "42b5af70",
      "metadata": {
        "id": "42b5af70"
      },
      "outputs": [],
      "source": [
        "# encode the text\n",
        "train_text_int = encode(train_text)\n",
        "test_text_int = encode(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "6458536d",
      "metadata": {
        "id": "6458536d",
        "outputId": "0e581198-2ef1-4a45-f87e-ac85b5807213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ero one after an anthrax attack was perpetrated on the company newspaper companies of the united states supermarket tabloids two\n",
            "\n",
            "ne two in addr arpa to its canonical name referrals icann org an ns record or name server record maps a domain name to a list of\n",
            "\n",
            "its transportation infrastructure asphalting new roads improving its ports and repairing war damaged roads and bridges since the\n",
            "\n",
            "y describing himself as merely a gold prospector who happened to find a nugget quote i believe the brain like any other organ ca\n",
            "\n",
            "ed following the workshop a panel of two eight experts worked to develop this report scientific evidence on condom effectiveness\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# sanity check: display a few random characters from the training text\n",
        "T = 128\n",
        "for _ in range(5):\n",
        "    # choose random position in text\n",
        "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
        "    print(train_text[N:N+T])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7724c34b",
      "metadata": {
        "id": "7724c34b"
      },
      "source": [
        "# Create a basic Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "f5955c3f",
      "metadata": {
        "id": "f5955c3f"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
        "    # create a basic Transformer model\n",
        "    model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
        "    # create a dummy input for initialization\n",
        "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
        "    # pass the dummy input to the model to initialize the parameters\n",
        "    params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
        "    return model, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "c3ea2291",
      "metadata": {
        "id": "c3ea2291"
      },
      "outputs": [],
      "source": [
        "# vocab size\n",
        "vocab_size= len(char_set)\n",
        "\n",
        "# internal model dimensions\n",
        "d_model=256\n",
        "\n",
        "# number of attention heads\n",
        "n_heads=8\n",
        "\n",
        "# number of Transformer layers\n",
        "n_layers=3\n",
        "\n",
        "# maximum sequence length\n",
        "max_len=128\n",
        "\n",
        "np.random.seed(42)          # affects get_batch sampling\n",
        "key = jax.random.key(42)    # affects model parameter initialization\n",
        "\n",
        "model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "c067e140",
      "metadata": {
        "id": "c067e140",
        "outputId": "09a80001-f095-4a32-c2f4-e434fe169b9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 2_413_312\n"
          ]
        }
      ],
      "source": [
        "# compute the number of parameters\n",
        "def count_params(params):\n",
        "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"Number of parameters: {count_params(params):_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "52231d73",
      "metadata": {
        "id": "52231d73",
        "outputId": "d57f3689-9a41-461b-f465-283a5fc063e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape: (4, 32)\n",
            "logits shape: (4, 32, 27)\n"
          ]
        }
      ],
      "source": [
        "# sanity check: create a batch of data & run a forward pass\n",
        "B, T = 4, 32\n",
        "batch = jax.random.randint(\n",
        "    key=key,\n",
        "    shape=(B, T), minval=0, maxval=len(char_set))\n",
        "logits = model.apply({\"params\": params}, batch)\n",
        "\n",
        "print(\"batch shape:\", batch.shape)  # (B, T)\n",
        "print(\"logits shape:\", logits.shape)  # (B, T, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0213425",
      "metadata": {
        "id": "b0213425"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "ab7a3479",
      "metadata": {
        "id": "ab7a3479"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "\n",
        "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
        "\n",
        "    Args:\n",
        "      logits: (B, T, V) float array of unnormalized scores.\n",
        "      targets: (B, T) integer array with ground-truth class ids.\n",
        "\n",
        "    Returns:\n",
        "      loss: scalar average cross-entropy over all positions.\n",
        "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
        "    \"\"\"\n",
        "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Per-position cross-entropy, then mean over all positions\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    loss = per_pos.mean()\n",
        "\n",
        "    # prediction over all positions\n",
        "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
        "\n",
        "    # compute accuracy over only the last position\n",
        "    is_match = preds == targets\n",
        "\n",
        "    # Accuracy over all positions\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "\n",
        "    # Accuracy over only last position\n",
        "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05221147",
      "metadata": {
        "id": "05221147"
      },
      "source": [
        "# Optimization step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "b720c846",
      "metadata": {
        "id": "b720c846"
      },
      "outputs": [],
      "source": [
        "# create an update function\n",
        "def train_step(params, opt_state, x, y, tx):\n",
        "    \"\"\"Single optimization step using optax optimizer.\n",
        "\n",
        "    Args:\n",
        "      params: pytree of model parameters.\n",
        "      opt_state: optax optimizer state corresponding to `params`.\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "      tx: optax.GradientTransformation (already initialized).\n",
        "\n",
        "    Returns:\n",
        "      new_params: updated parameters after one gradient step.\n",
        "      new_opt_state: updated optimizer state.\n",
        "      metrics: dict of scalar metrics (loss, acc).\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply({\"params\": params}, x)\n",
        "        loss, metrics = loss_and_metrics(logits, y)\n",
        "        return loss, metrics\n",
        "\n",
        "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "\n",
        "    # optax update: compute parameter updates and new optimizer state\n",
        "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, metrics\n",
        "\n",
        "# jit: last argument should be static because it is an object\n",
        "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269d8e59",
      "metadata": {
        "id": "269d8e59"
      },
      "source": [
        "# Batch creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "beb30f4e",
      "metadata": {
        "id": "beb30f4e"
      },
      "outputs": [],
      "source": [
        "# create a batch from the training data\n",
        "def get_batch(text_int, B, T):\n",
        "    \"\"\"Create a random batch of data from text_int.\n",
        "\n",
        "    Args:\n",
        "      text_int: 1D array of token ids.\n",
        "      B: batch size (number of sequences).\n",
        "      T: sequence length (number of tokens per sequence).\n",
        "\n",
        "    Returns:\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "    \"\"\"\n",
        "    # choose random starting indices for each sequence in the batch\n",
        "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
        "    # inputs are text from i to i+T\n",
        "    x = np.stack([text_int[i:i+T] for i in ix])\n",
        "    # targets are text from i+1 to i+T+1\n",
        "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
        "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da80db",
      "metadata": {
        "id": "f5da80db"
      },
      "source": [
        "# Optimizer creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "340e8a4c",
      "metadata": {
        "id": "340e8a4c",
        "outputId": "83c90e65-8160-4119-c0b2-e41de49d7e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized optimizer: Adam lr=0.001\n"
          ]
        }
      ],
      "source": [
        "# define optax optimizer\n",
        "learning_rate = 0.001\n",
        "# Create Adam optimizer (Optax)\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "# Initialize optimizer state for current params\n",
        "opt_state = tx.init(params)\n",
        "print(f\"Initialized optimizer: Adam lr={learning_rate}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f2f1d0",
      "metadata": {
        "id": "28f2f1d0",
        "outputId": "59e40493-9c28-42d9-a8db-229b14e4328d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0  time: 4.0 seconds\n",
            "\t \t loss(train :: test): 3.7973 :: 3.5791\n",
            "\t \t accuracy (train :: test): 1.3% :: 17.1%\n",
            "\t \t accuracy (last character) (train :: test): 0.8% :: 18.7%\n",
            "\n",
            "iteration 200  time: 8.5 seconds\n",
            "\t \t loss(train :: test): 2.0606 :: 2.0480\n",
            "\t \t accuracy (train :: test): 38.0% :: 37.7%\n",
            "\t \t accuracy (last character) (train :: test): 37.5% :: 35.1%\n",
            "\n",
            "iteration 400  time: 13.2 seconds\n",
            "\t \t loss(train :: test): 1.6994 :: 1.7725\n",
            "\t \t accuracy (train :: test): 47.8% :: 45.9%\n",
            "\t \t accuracy (last character) (train :: test): 49.2% :: 43.0%\n",
            "\n",
            "iteration 600  time: 17.7 seconds\n",
            "\t \t loss(train :: test): 1.6878 :: 1.6663\n",
            "\t \t accuracy (train :: test): 48.1% :: 49.1%\n",
            "\t \t accuracy (last character) (train :: test): 55.5% :: 53.4%\n",
            "\n",
            "iteration 800  time: 22.2 seconds\n",
            "\t \t loss(train :: test): 1.5882 :: 1.6131\n",
            "\t \t accuracy (train :: test): 51.6% :: 50.6%\n",
            "\t \t accuracy (last character) (train :: test): 49.2% :: 53.3%\n",
            "\n",
            "iteration 1_000  time: 26.7 seconds\n",
            "\t \t loss(train :: test): 1.5070 :: 1.5377\n",
            "\t \t accuracy (train :: test): 52.2% :: 52.7%\n",
            "\t \t accuracy (last character) (train :: test): 48.4% :: 52.5%\n",
            "\n",
            "iteration 1_200  time: 31.2 seconds\n",
            "\t \t loss(train :: test): 1.4827 :: 1.5446\n",
            "\t \t accuracy (train :: test): 53.9% :: 52.5%\n",
            "\t \t accuracy (last character) (train :: test): 54.7% :: 55.8%\n",
            "\n",
            "iteration 1_400  time: 35.8 seconds\n",
            "\t \t loss(train :: test): 1.4481 :: 1.5168\n",
            "\t \t accuracy (train :: test): 54.9% :: 53.0%\n",
            "\t \t accuracy (last character) (train :: test): 60.9% :: 56.6%\n",
            "\n",
            "iteration 1_600  time: 40.3 seconds\n",
            "\t \t loss(train :: test): 1.4445 :: 1.5233\n",
            "\t \t accuracy (train :: test): 55.0% :: 53.0%\n",
            "\t \t accuracy (last character) (train :: test): 52.3% :: 55.8%\n",
            "\n",
            "iteration 1_800  time: 44.9 seconds\n",
            "\t \t loss(train :: test): 1.4696 :: 1.5033\n",
            "\t \t accuracy (train :: test): 54.4% :: 53.2%\n",
            "\t \t accuracy (last character) (train :: test): 53.9% :: 54.9%\n",
            "\n",
            "iteration 2_000  time: 49.5 seconds\n",
            "\t \t loss(train :: test): 1.5038 :: 1.4894\n",
            "\t \t accuracy (train :: test): 53.2% :: 53.7%\n",
            "\t \t accuracy (last character) (train :: test): 52.3% :: 56.2%\n",
            "\n",
            "iteration 2_200  time: 54.1 seconds\n",
            "\t \t loss(train :: test): 1.5129 :: 1.4967\n",
            "\t \t accuracy (train :: test): 53.9% :: 53.4%\n",
            "\t \t accuracy (last character) (train :: test): 52.3% :: 57.0%\n",
            "\n",
            "iteration 2_400  time: 58.8 seconds\n",
            "\t \t loss(train :: test): 1.4314 :: 1.4322\n",
            "\t \t accuracy (train :: test): 55.0% :: 55.3%\n",
            "\t \t accuracy (last character) (train :: test): 56.2% :: 57.4%\n",
            "\n",
            "iteration 2_600  time: 63.4 seconds\n",
            "\t \t loss(train :: test): 1.4613 :: 1.4614\n",
            "\t \t accuracy (train :: test): 54.8% :: 54.5%\n",
            "\t \t accuracy (last character) (train :: test): 55.5% :: 57.5%\n",
            "\n",
            "iteration 2_800  time: 68.1 seconds\n",
            "\t \t loss(train :: test): 1.4596 :: 1.4494\n",
            "\t \t accuracy (train :: test): 53.8% :: 54.8%\n",
            "\t \t accuracy (last character) (train :: test): 53.1% :: 57.2%\n",
            "\n",
            "iteration 3_000  time: 72.7 seconds\n",
            "\t \t loss(train :: test): 1.4490 :: 1.4178\n",
            "\t \t accuracy (train :: test): 55.5% :: 55.8%\n",
            "\t \t accuracy (last character) (train :: test): 53.1% :: 60.0%\n",
            "\n",
            "iteration 3_200  time: 77.3 seconds\n",
            "\t \t loss(train :: test): 1.4187 :: 1.4322\n",
            "\t \t accuracy (train :: test): 56.4% :: 55.3%\n",
            "\t \t accuracy (last character) (train :: test): 60.2% :: 60.1%\n",
            "\n",
            "iteration 3_400  time: 82.1 seconds\n",
            "\t \t loss(train :: test): 1.4299 :: 1.4429\n",
            "\t \t accuracy (train :: test): 55.5% :: 55.2%\n",
            "\t \t accuracy (last character) (train :: test): 57.8% :: 59.7%\n",
            "\n",
            "iteration 3_600  time: 86.8 seconds\n",
            "\t \t loss(train :: test): 1.4330 :: 1.4302\n",
            "\t \t accuracy (train :: test): 55.0% :: 55.6%\n",
            "\t \t accuracy (last character) (train :: test): 61.7% :: 56.6%\n",
            "\n",
            "iteration 3_800  time: 91.5 seconds\n",
            "\t \t loss(train :: test): 1.4485 :: 1.4169\n",
            "\t \t accuracy (train :: test): 55.1% :: 55.8%\n",
            "\t \t accuracy (last character) (train :: test): 59.4% :: 57.9%\n",
            "\n",
            "iteration 4_000  time: 96.3 seconds\n",
            "\t \t loss(train :: test): 1.3799 :: 1.4335\n",
            "\t \t accuracy (train :: test): 56.4% :: 55.4%\n",
            "\t \t accuracy (last character) (train :: test): 57.8% :: 57.9%\n",
            "\n",
            "iteration 4_200  time: 101.1 seconds\n",
            "\t \t loss(train :: test): 1.4032 :: 1.4288\n",
            "\t \t accuracy (train :: test): 55.7% :: 55.4%\n",
            "\t \t accuracy (last character) (train :: test): 61.7% :: 58.2%\n",
            "\n",
            "iteration 4_400  time: 105.9 seconds\n",
            "\t \t loss(train :: test): 1.3607 :: 1.4324\n",
            "\t \t accuracy (train :: test): 57.2% :: 55.3%\n",
            "\t \t accuracy (last character) (train :: test): 58.6% :: 58.5%\n",
            "\n",
            "iteration 4_600  time: 110.8 seconds\n",
            "\t \t loss(train :: test): 1.3670 :: 1.4326\n",
            "\t \t accuracy (train :: test): 57.3% :: 55.2%\n",
            "\t \t accuracy (last character) (train :: test): 58.6% :: 58.6%\n",
            "\n",
            "iteration 4_800  time: 115.7 seconds\n",
            "\t \t loss(train :: test): 1.4412 :: 1.4171\n",
            "\t \t accuracy (train :: test): 55.8% :: 55.8%\n",
            "\t \t accuracy (last character) (train :: test): 63.3% :: 57.0%\n",
            "\n",
            "iteration 5_000  time: 120.7 seconds\n",
            "\t \t loss(train :: test): 1.3799 :: 1.4196\n",
            "\t \t accuracy (train :: test): 56.7% :: 55.9%\n",
            "\t \t accuracy (last character) (train :: test): 62.5% :: 57.4%\n",
            "\n",
            "iteration 5_200  time: 125.6 seconds\n",
            "\t \t loss(train :: test): 1.3868 :: 1.4000\n",
            "\t \t accuracy (train :: test): 56.4% :: 56.3%\n",
            "\t \t accuracy (last character) (train :: test): 57.0% :: 59.3%\n",
            "\n",
            "iteration 5_400  time: 130.5 seconds\n",
            "\t \t loss(train :: test): 1.3776 :: 1.4011\n",
            "\t \t accuracy (train :: test): 56.7% :: 56.1%\n",
            "\t \t accuracy (last character) (train :: test): 57.8% :: 56.2%\n",
            "\n",
            "iteration 5_600  time: 135.4 seconds\n",
            "\t \t loss(train :: test): 1.3640 :: 1.4051\n",
            "\t \t accuracy (train :: test): 57.1% :: 56.3%\n",
            "\t \t accuracy (last character) (train :: test): 61.7% :: 58.1%\n",
            "\n",
            "iteration 5_800  time: 140.3 seconds\n",
            "\t \t loss(train :: test): 1.4070 :: 1.3698\n",
            "\t \t accuracy (train :: test): 55.4% :: 56.9%\n",
            "\t \t accuracy (last character) (train :: test): 60.2% :: 60.6%\n",
            "\n",
            "iteration 6_000  time: 145.1 seconds\n",
            "\t \t loss(train :: test): 1.3745 :: 1.3803\n",
            "\t \t accuracy (train :: test): 56.7% :: 57.2%\n",
            "\t \t accuracy (last character) (train :: test): 59.4% :: 57.9%\n",
            "\n",
            "iteration 6_200  time: 149.9 seconds\n",
            "\t \t loss(train :: test): 1.3826 :: 1.3981\n",
            "\t \t accuracy (train :: test): 56.2% :: 56.3%\n",
            "\t \t accuracy (last character) (train :: test): 57.0% :: 60.5%\n",
            "\n",
            "iteration 6_400  time: 154.8 seconds\n",
            "\t \t loss(train :: test): 1.3529 :: 1.4029\n",
            "\t \t accuracy (train :: test): 58.5% :: 55.8%\n",
            "\t \t accuracy (last character) (train :: test): 63.3% :: 57.8%\n",
            "\n",
            "iteration 6_600  time: 159.6 seconds\n",
            "\t \t loss(train :: test): 1.3484 :: 1.3676\n",
            "\t \t accuracy (train :: test): 57.6% :: 57.2%\n",
            "\t \t accuracy (last character) (train :: test): 57.8% :: 57.9%\n",
            "\n",
            "iteration 6_800  time: 164.4 seconds\n",
            "\t \t loss(train :: test): 1.3588 :: 1.3992\n",
            "\t \t accuracy (train :: test): 57.3% :: 56.2%\n",
            "\t \t accuracy (last character) (train :: test): 52.3% :: 60.7%\n",
            "\n",
            "iteration 7_000  time: 169.2 seconds\n",
            "\t \t loss(train :: test): 1.3308 :: 1.3613\n",
            "\t \t accuracy (train :: test): 57.5% :: 57.0%\n",
            "\t \t accuracy (last character) (train :: test): 58.6% :: 62.4%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "niter = 10000\n",
        "B, T = 128, 32\n",
        "\n",
        "# ==== LOGGING: init (place here, after learning_rate / niter / B / T are set) ====\n",
        "run_tag = \"baseline_T32_B128_lr1e-3\"   # rename per run each experiment\n",
        "seed = 42\n",
        "cfg = dict(\n",
        "    run_tag=run_tag, seed=seed,\n",
        "    B=B, T=T, steps=niter,                 # match your loop exactly\n",
        "    vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, max_len=max_len,\n",
        "    lr=learning_rate, optimizer=\"Adam\"\n",
        ")\n",
        "logger = ExperimentLogger(root=\"runs\", run_tag=run_tag)\n",
        "logger.log_config(cfg)\n",
        "# ==== /init ====\n",
        "\n",
        "loss_history = []\n",
        "time_history = []\n",
        "time_test_history = []\n",
        "loss_test_history = []\n",
        "time_start = time.time()\n",
        "for it in range(niter):\n",
        "    batch = get_batch(train_text_int, B, T)\n",
        "    input, target = batch[0], batch[1]\n",
        "    params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
        "\n",
        "    # update params and opt_state\n",
        "    params = params_new\n",
        "    opt_state = opt_state_new\n",
        "    acc = metrics['acc']\n",
        "    acc_last = metrics['acc_last']\n",
        "    loss = metrics['loss']\n",
        "\n",
        "    loss_history.append(loss)\n",
        "    time_history.append(time.time() - time_start)\n",
        "\n",
        "    if it % (niter // 50) == 0 or it == niter - 1:\n",
        "        time_since_start = time.time() - time_start\n",
        "        # compute loss on test set\n",
        "        B_test, T_test = 1024, 32\n",
        "        test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "        test_input, test_target = test_batch[0], test_batch[1]\n",
        "        test_logits = model.apply({\"params\": params}, test_input)\n",
        "        test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
        "        test_acc = test_metrics['acc']\n",
        "        test_acc_last = test_metrics['acc_last']\n",
        "        loss_test_history.append(test_loss)\n",
        "        time_test_history.append(time_since_start)\n",
        "        print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
        "        print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
        "        print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
        "        print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
        "        print()\n",
        "        # ==== LOGGING: record eval ====\n",
        "        logger.log_eval(\n",
        "            step=it,\n",
        "            val_loss=float(test_loss),\n",
        "            val_acc=float(test_acc),\n",
        "            val_acc_last=float(test_acc_last)\n",
        "        )\n",
        "        # ==== /LOGGING ====\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LOGGING: finalize ====\n",
        "final_val_loss = float(loss_test_history[-1]) if len(loss_test_history)>0 else float(loss_history[-1])\n",
        "final_val_bpc  = final_val_loss / float(jnp.log(2.0))\n",
        "# reuse last printed test metrics if available; otherwise do a small eval now\n",
        "try:\n",
        "    final_val_acc = float(test_acc)\n",
        "    final_val_acc_last = float(test_acc_last)\n",
        "    final_step = it\n",
        "except NameError:\n",
        "    B_eval, T_eval = 512, T\n",
        "    test_batch = get_batch(test_text_int, B_eval, T_eval)\n",
        "    test_logits = model.apply({\"params\": params}, test_batch[0])\n",
        "    test_loss, test_metrics = loss_and_metrics(test_logits, test_batch[1])\n",
        "    final_val_loss = float(test_loss); final_val_bpc = final_val_loss / float(jnp.log(2.0))\n",
        "    final_val_acc = float(test_metrics['acc']); final_val_acc_last = float(test_metrics['acc_last'])\n",
        "    final_step = niter\n",
        "\n",
        "\n",
        "\n",
        "logger.finish(\n",
        "    params_count=count_params(params),\n",
        "    final_row=dict(\n",
        "        val_loss_final=final_val_loss,\n",
        "        val_bpc_final=final_val_bpc,\n",
        "        val_acc_final=final_val_acc,\n",
        "        val_acc_last_final=final_val_acc_last,\n",
        "        step_final=final_step\n",
        "    )\n",
        ")\n",
        "# ==== /finalize ====\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yoy61nsQ7V9T",
        "outputId": "21fd089a-1a3b-4578-8ec9-0378c981c074"
      },
      "id": "Yoy61nsQ7V9T",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Wrote master row → runs/experiments.csv\n",
            "[LOG] Artifacts in → runs/2025-10-28_09-02-00_baseline_T32_B128_lr1e-3_a9c0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "217fb628",
      "metadata": {
        "id": "217fb628"
      },
      "outputs": [],
      "source": [
        "# plot the loss history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure() # <<< add\n",
        "\n",
        "plt.plot(time_history, loss_history, '-', label='train', color=\"blue\")\n",
        "plt.plot(time_test_history, loss_test_history, '-', label='test', lw=2, color=\"red\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Training Loss History\")\n",
        "plt.grid()\n",
        "\n",
        "# SAVE to the current run folder\n",
        "plot_path_png = str(logger.run_dir / \"loss_history.png\")   # <<< add\n",
        "plt.savefig(plot_path_png, dpi=200, bbox_inches='tight')   # <<< add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "fbb0b0d1",
      "metadata": {
        "id": "fbb0b0d1",
        "outputId": "19649563-7711-493f-9726-da45d41c838e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated ids shape: (1, 1000)\n",
            "generated text:\n",
            "hello my friend socialism electromagnetic connecting behavioured in one construction and township of their translation two seven trilogy and water are known as the massachusetts for service constitution of the one nine eight zero s the consuls of the city of the collection of the subspecial sea rodin cabride the world s all of the three nine eight four three four six seven seven changing but the chinese field along with any education products within a common participated in a modern general policy with east king him for his fans and several percentage or that it community some internet principle you the salin recipient of the country fairly see the rationality of apiral in this subsequent decreased accounts of the problem of political military and prayer language for gas would be able to status was a created community and mandatory and are still used to do not the christian however model and remove however the summer of the first week with supported the same that the name the shape of the real the\n"
          ]
        }
      ],
      "source": [
        "B = 1\n",
        "seed = 42\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "prompt = \"hello my fri\"\n",
        "# prompt_int = encode(prompt.lower())\n",
        "prompt_int = jnp.array([ [char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]] ], dtype=jnp.int32)\n",
        "\n",
        "gen_len = 1000\n",
        "out_ids = generation.generate_tokens(model, params, rng, prompt_int, gen_len, block_size=64,\n",
        "                          temperature=0.7, sample=True)\n",
        "print('generated ids shape:', out_ids.shape)\n",
        "print('generated text:')\n",
        "generated_text = ''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0]))\n",
        "# concatenate with prompt\n",
        "print(prompt + generated_text)\n",
        "#print(''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0])))\n",
        "\n",
        "# save the sample now that we have it\n",
        "logger.save_sample(prompt, prompt + generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"CWD:\", os.getcwd())              # should show .../char-llm-assignment/char-llm-assignment/...\n",
        "assert os.path.isdir(\"runs\"), \"No 'runs' folder here.\"\n",
        "\n",
        "zip_path = \"/content/runs_export.zip\"\n",
        "shutil.make_archive(\"/content/runs_export\", \"zip\", \"runs\")  # zip the local ./runs\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "Bj4Oxva5APj5",
        "outputId": "809e14eb-0869-4de0-f38b-86d1d327eb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "id": "Bj4Oxva5APj5",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /content/char-llm-assignment/char-llm-assignment/char-llm-assignment/char-llm-assignment\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_778ff6e0-e8e8-4817-a237-72cf047f9cab\", \"runs_export.zip\", 63634)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}