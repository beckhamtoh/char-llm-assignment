{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "LNS1ztqUUrw7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNS1ztqUUrw7",
        "outputId": "21ffc255-cbb8-406b-e302-508d14d5848f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'char-llm-assignment'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 12 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 30.14 MiB | 32.11 MiB/s, done.\n",
            "/content/char-llm-assignment\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/beckhamtoh/char-llm-assignment.git\n",
        "%cd char-llm-assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e2def0f4",
      "metadata": {
        "id": "e2def0f4"
      },
      "outputs": [],
      "source": [
        "# Enable autoreload of local Python modules (e.g., models)\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "# manual reload for local modules\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5001b05",
      "metadata": {
        "id": "d5001b05"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import time\n",
        "\n",
        "# local imports\n",
        "import models.models as models\n",
        "import util.generation as generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "906db4f2",
      "metadata": {
        "id": "906db4f2"
      },
      "outputs": [],
      "source": [
        "# initialize the jax random key\n",
        "key = jax.random.key(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762a1c8f",
      "metadata": {
        "id": "762a1c8f"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "86825275",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86825275",
        "outputId": "51b635db-0434-44d4-ccc9-aa0b1093834e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of training text: 90_000_000 characters\n",
            "Length of test text: 5_000_000 characters\n"
          ]
        }
      ],
      "source": [
        "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
        "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
        "    train_text = f.read()\n",
        "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
        "    test_text = f.read()\n",
        "\n",
        "# print the length of the training text and test text\n",
        "print(f\"Length of training text: {len(train_text):_} characters\")\n",
        "print(f\"Length of test text: {len(test_text):_} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfdca63",
      "metadata": {
        "id": "7bfdca63"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "34b1eafe",
      "metadata": {
        "id": "34b1eafe"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary (lowercase + space + a few punctuations)\n",
        "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
        "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
        "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
        "\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to array of integers\"\"\"\n",
        "    ids = [char_to_int[c] for c in s]\n",
        "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "42b5af70",
      "metadata": {
        "id": "42b5af70"
      },
      "outputs": [],
      "source": [
        "# encode the text\n",
        "train_text_int = encode(train_text)\n",
        "test_text_int = encode(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6458536d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6458536d",
        "outputId": "f7572de8-c91b-4039-cedd-c6aab21535c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "of the illness and went to the fragrant mountain to give thanks to the person when he discovered that his own daughter gave up h\n",
            "\n",
            "together they performed the arcade fire s song wake up from their album funeral he joined them again on one five september singi\n",
            "\n",
            "ro asiatic language phylum its closest relatives are the berber semitic and beja groups of languages written records of the egyp\n",
            "\n",
            "es were produced between about one nine three zero and one nine three five but the concept was abandonded because of its limited\n",
            "\n",
            "xt is read aloud twice during the celebration setting the biblical book of esther is set in the third year of ahasuerus a king o\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# sanity check: display a few random characters from the training text\n",
        "T = 128\n",
        "for _ in range(5):\n",
        "    # choose random position in text\n",
        "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
        "    print(train_text[N:N+T])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7724c34b",
      "metadata": {
        "id": "7724c34b"
      },
      "source": [
        "# Create a basic Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f5955c3f",
      "metadata": {
        "id": "f5955c3f"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
        "    # create a basic Transformer model\n",
        "    model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
        "    # create a dummy input for initialization\n",
        "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
        "    # pass the dummy input to the model to initialize the parameters\n",
        "    params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
        "    return model, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c3ea2291",
      "metadata": {
        "id": "c3ea2291"
      },
      "outputs": [],
      "source": [
        "# vocab size\n",
        "vocab_size= len(char_set)\n",
        "\n",
        "# internal model dimensions\n",
        "d_model=256\n",
        "\n",
        "# number of attention heads\n",
        "n_heads=8\n",
        "\n",
        "# number of Transformer layers\n",
        "n_layers=2\n",
        "\n",
        "# maximum sequence length\n",
        "max_len=128\n",
        "\n",
        "model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c067e140",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c067e140",
        "outputId": "93b52b7a-849a-4ef0-d784-6196a8832d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 1_624_576\n"
          ]
        }
      ],
      "source": [
        "# compute the number of parameters\n",
        "def count_params(params):\n",
        "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"Number of parameters: {count_params(params):_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "52231d73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52231d73",
        "outputId": "07cd9c86-6d8a-45d4-b7c6-497aa701c004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch shape: (4, 32)\n",
            "logits shape: (4, 32, 27)\n"
          ]
        }
      ],
      "source": [
        "# sanity check: create a batch of data & run a forward pass\n",
        "B, T = 4, 32\n",
        "batch = jax.random.randint(\n",
        "    key=key,\n",
        "    shape=(B, T), minval=0, maxval=len(char_set))\n",
        "logits = model.apply({\"params\": params}, batch)\n",
        "\n",
        "print(\"batch shape:\", batch.shape)  # (B, T)\n",
        "print(\"logits shape:\", logits.shape)  # (B, T, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0213425",
      "metadata": {
        "id": "b0213425"
      },
      "source": [
        "# Loss function\n",
        "\n",
        "Original code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ab7a3479",
      "metadata": {
        "id": "ab7a3479"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "\n",
        "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
        "\n",
        "    Args:\n",
        "      logits: (B, T, V) float array of unnormalized scores.\n",
        "      targets: (B, T) integer array with ground-truth class ids.\n",
        "\n",
        "    Returns:\n",
        "      loss: scalar average cross-entropy over all positions.\n",
        "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
        "    \"\"\"\n",
        "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Per-position cross-entropy, then mean over all positions\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    loss = per_pos.mean()\n",
        "\n",
        "    # prediction over all positions\n",
        "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
        "\n",
        "    # compute accuracy over only the last position\n",
        "    is_match = preds == targets\n",
        "\n",
        "    # Accuracy over all positions\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "\n",
        "    # Accuracy over only last position\n",
        "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba318fe",
      "metadata": {},
      "source": [
        "### Loss everywhere vs. loss at last token\n",
        "\n",
        "last_token_only = False vs last_token_only = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228d9145",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets, last_token_only=False):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "\n",
        "    Args:\n",
        "      logits: (B, T, V) float array of unnormalized scores.\n",
        "      targets: (B, T) integer array with ground-truth class ids.\n",
        "      last_token_only: If True, compute loss only on the last token position.\n",
        "\n",
        "    Returns:\n",
        "      loss: scalar average cross-entropy.\n",
        "      metrics: dict with keys \"loss\", \"acc\", and \"acc_last\".\n",
        "    \"\"\"\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Compute per-position cross-entropy\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    \n",
        "    if last_token_only:\n",
        "        # Reshape back to (B, T) and take only last token\n",
        "        per_pos = per_pos.reshape(targets.shape)  # (B, T)\n",
        "        loss = per_pos[:, -1].mean()  # Average over batch, last position only\n",
        "    else:\n",
        "        # Original behavior: average over all positions\n",
        "        loss = per_pos.mean()\n",
        "\n",
        "    # Predictions and accuracy (unchanged)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    is_match = preds == targets\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "    acc_last = jnp.mean(is_match[:, -1].astype(jnp.float32))\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96b0957",
      "metadata": {},
      "source": [
        "# Label smoothing\n",
        "\n",
        "- label_smoothing = 0.0 - baseline (hard labels)\n",
        "- label_smoothing = 0.05 - light smoothing\n",
        "- label_smoothing = 0.1 - standard smoothing (common choice)\n",
        "- label_smoothing = 0.2 - heavier smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098ebe39",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets, label_smoothing=0.0):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\"\"\"\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Always use one-hot encoding, label_smoothing=0.0 becomes hard labels\n",
        "    one_hot_targets = jax.nn.one_hot(flat_targets, vocab)\n",
        "    smooth_targets = one_hot_targets * (1 - label_smoothing) + label_smoothing / vocab\n",
        "    per_pos = optax.softmax_cross_entropy(flat_logits, smooth_targets)\n",
        "    \n",
        "    loss = per_pos.mean()\n",
        "\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    is_match = preds == targets\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "    acc_last = jnp.mean(is_match[:, -1].astype(jnp.float32))\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582ffaee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified train_step that supports label smoothing\n",
        "def train_step(params, opt_state, x, y, tx, label_smoothing=0.0):\n",
        "    \"\"\"Single optimization step using optax optimizer.\n",
        "\n",
        "    Args:\n",
        "      params: pytree of model parameters.\n",
        "      opt_state: optax optimizer state corresponding to `params`.\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "      tx: optax.GradientTransformation (already initialized).\n",
        "      label_smoothing: Float in [0, 1] for label smoothing.\n",
        "\n",
        "    Returns:\n",
        "      new_params: updated parameters after one gradient step.\n",
        "      new_opt_state: updated optimizer state.\n",
        "      metrics: dict of scalar metrics (loss, acc).\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply({\"params\": params}, x)\n",
        "        loss, metrics = loss_and_metrics(logits, y, label_smoothing=label_smoothing)\n",
        "        return loss, metrics\n",
        "\n",
        "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "\n",
        "    # optax update: compute parameter updates and new optimizer state\n",
        "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, metrics\n",
        "\n",
        "# jit: mark both tx and label_smoothing as static\n",
        "train_step = jax.jit(train_step, static_argnames=(\"tx\", \"label_smoothing\"))\n",
        "\n",
        "# create a batch from the training data\n",
        "def get_batch(text_int, B, T):\n",
        "    \"\"\"Create a random batch of data from text_int.\n",
        "\n",
        "    Args:\n",
        "      text_int: 1D array of token ids.\n",
        "      B: batch size (number of sequences).\n",
        "      T: sequence length (number of tokens per sequence).\n",
        "\n",
        "    Returns:\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "    \"\"\"\n",
        "    # choose random starting indices for each sequence in the batch\n",
        "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
        "    # inputs are text from i to i+T\n",
        "    x = np.stack([text_int[i:i+T] for i in ix])\n",
        "    # targets are text from i+1 to i+T+1\n",
        "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
        "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)\n",
        "\n",
        "# define optax optimizer\n",
        "learning_rate = 0.001\n",
        "# Create Adam optimizer (Optax)\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "# Initialize optimizer state for current params\n",
        "opt_state = tx.init(params)\n",
        "print(f\"Initialized optimizer: Adam lr={learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e192252",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_label_smoothing(model, params_init, opt_state_init, tx, train_text_int, \n",
        "                                test_text_int, train_step_fn, get_batch, \n",
        "                                label_smoothing_values, niter=5000, B=128, T=32):\n",
        "    \"\"\"\n",
        "    Train model with different label smoothing values and collect results.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        params_init: Initial model parameters\n",
        "        opt_state_init: Initial optimizer state\n",
        "        tx: Optimizer\n",
        "        train_text_int: Training data\n",
        "        test_text_int: Test data\n",
        "        train_step_fn: Training step function (should accept label_smoothing)\n",
        "        get_batch: Function to get batches\n",
        "        label_smoothing_values: List of label smoothing values to try\n",
        "        niter: Number of training iterations\n",
        "        B: Batch size\n",
        "        T: Sequence length\n",
        "    \n",
        "    Returns:\n",
        "        results: Dictionary with training histories for each smoothing value\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for ls_val in label_smoothing_values:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training with label_smoothing = {ls_val}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        # Reset model parameters and optimizer state for each run\n",
        "        params = params_init\n",
        "        opt_state = opt_state_init\n",
        "        \n",
        "        loss_history = []\n",
        "        time_history = []\n",
        "        loss_test_history = []\n",
        "        acc_test_history = []\n",
        "        acc_last_test_history = []\n",
        "        time_test_history = []\n",
        "        \n",
        "        time_start = time.time()\n",
        "        \n",
        "        for it in range(niter):\n",
        "            batch = get_batch(train_text_int, B, T)\n",
        "            input, target = batch[0], batch[1]\n",
        "            \n",
        "            # Modified train_step to accept label_smoothing\n",
        "            params_new, opt_state_new, metrics = train_step_fn(\n",
        "                params, opt_state, input, target, tx, label_smoothing=ls_val  \n",
        "            )\n",
        "            \n",
        "            params = params_new\n",
        "            opt_state = opt_state_new\n",
        "            acc = metrics['acc']\n",
        "            acc_last = metrics['acc_last']\n",
        "            loss = metrics['loss']\n",
        "            \n",
        "            loss_history.append(float(loss))\n",
        "            time_history.append(time.time() - time_start)\n",
        "            \n",
        "            if it % (niter // 50) == 0 or it == niter - 1:\n",
        "                time_since_start = time.time() - time_start\n",
        "                \n",
        "                # Compute loss on test set\n",
        "                B_test, T_test = 1024, 32\n",
        "                test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "                test_input, test_target = test_batch[0], test_batch[1]\n",
        "                test_logits = model.apply({\"params\": params}, test_input)\n",
        "                \n",
        "                # Use same label_smoothing for test evaluation\n",
        "                test_loss, test_metrics = loss_and_metrics(\n",
        "                    test_logits, test_target, label_smoothing=ls_val\n",
        "                )\n",
        "                test_acc = test_metrics['acc']\n",
        "                test_acc_last = test_metrics['acc_last']\n",
        "                \n",
        "                loss_test_history.append(float(test_loss))\n",
        "                acc_test_history.append(float(test_acc))\n",
        "                acc_last_test_history.append(float(test_acc_last))\n",
        "                time_test_history.append(time_since_start)\n",
        "                \n",
        "                print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
        "                print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
        "                print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
        "                print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
        "                print()\n",
        "        \n",
        "        results[ls_val] = {\n",
        "            'loss_train': loss_history,\n",
        "            'loss_test': loss_test_history,\n",
        "            'acc_test': acc_test_history,\n",
        "            'acc_last_test': acc_last_test_history,\n",
        "            'time_train': time_history,\n",
        "            'time_test': time_test_history\n",
        "        }\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a881da74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_label_smoothing_comparison(results):\n",
        "    \"\"\"\n",
        "    Plot comparison of different label smoothing values.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary returned by train_with_label_smoothing()\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Label Smoothing Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    labels_map = {\n",
        "        0.0: 'Baseline (0.0)',\n",
        "        0.05: 'Light (0.05)',\n",
        "        0.1: 'Standard (0.1)',\n",
        "        0.2: 'Heavy (0.2)'\n",
        "    }\n",
        "    \n",
        "    # Plot 1: Training Loss over time\n",
        "    ax1 = axes[0, 0]\n",
        "    for idx, (ls_val, data) in enumerate(results.items()):\n",
        "        ax1.plot(data['time_train'], data['loss_train'], \n",
        "                label=labels_map.get(ls_val, f'{ls_val}'), linewidth=1.5, \n",
        "                color=colors[idx % len(colors)], alpha=0.7)\n",
        "    ax1.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
        "    ax1.set_title('Training Loss vs Time', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Test Loss\n",
        "    ax2 = axes[0, 1]\n",
        "    for idx, (ls_val, data) in enumerate(results.items()):\n",
        "        ax2.plot(data['time_test'], data['loss_test'], \n",
        "                label=labels_map.get(ls_val, f'{ls_val}'), linewidth=2, \n",
        "                marker='o', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax2.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax2.set_ylabel('Test Loss', fontsize=12)\n",
        "    ax2.set_title('Test Loss vs Time', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Test Accuracy\n",
        "    ax3 = axes[1, 0]\n",
        "    for idx, (ls_val, data) in enumerate(results.items()):\n",
        "        acc_percent = [a * 100 for a in data['acc_test']]\n",
        "        ax3.plot(data['time_test'], acc_percent, \n",
        "                label=labels_map.get(ls_val, f'{ls_val}'), linewidth=2, \n",
        "                marker='s', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax3.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax3.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    ax3.set_title('Overall Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax3.legend(fontsize=10)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Last Token Accuracy\n",
        "    ax4 = axes[1, 1]\n",
        "    for idx, (ls_val, data) in enumerate(results.items()):\n",
        "        acc_last_percent = [a * 100 for a in data['acc_last_test']]\n",
        "        ax4.plot(data['time_test'], acc_last_percent, \n",
        "                label=labels_map.get(ls_val, f'{ls_val}'), linewidth=2, \n",
        "                marker='^', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax4.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax4.set_ylabel('Last Token Accuracy (%)', fontsize=12)\n",
        "    ax4.set_title('Last Token Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax4.legend(fontsize=10)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_final_comparison(results):\n",
        "    \"\"\"\n",
        "    Create bar charts comparing final metrics.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle('Final Metrics Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    smoothing_vals = list(results.keys())\n",
        "    labels = [f'{v}' for v in smoothing_vals]\n",
        "    \n",
        "    # Extract final values\n",
        "    final_loss = [results[v]['loss_test'][-1] for v in smoothing_vals]\n",
        "    final_acc = [results[v]['acc_test'][-1] * 100 for v in smoothing_vals]\n",
        "    final_acc_last = [results[v]['acc_last_test'][-1] * 100 for v in smoothing_vals]\n",
        "    \n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    # Plot 1: Final Test Loss\n",
        "    ax1 = axes[0]\n",
        "    bars1 = ax1.bar(labels, final_loss, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax1.set_xlabel('Label Smoothing', fontsize=12)\n",
        "    ax1.set_ylabel('Test Loss', fontsize=12)\n",
        "    ax1.set_title('Final Test Loss', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Plot 2: Final Test Accuracy\n",
        "    ax2 = axes[1]\n",
        "    bars2 = ax2.bar(labels, final_acc, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax2.set_xlabel('Label Smoothing', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title('Final Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    ax2.set_ylim([0, 100])\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Plot 3: Final Last Token Accuracy\n",
        "    ax3 = axes[2]\n",
        "    bars3 = ax3.bar(labels, final_acc_last, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax3.set_xlabel('Label Smoothing', fontsize=12)\n",
        "    ax3.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax3.set_title('Final Last Token Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "    ax3.set_ylim([0, 100])\n",
        "    for bar in bars3:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def print_summary(results):\n",
        "    \"\"\"Print summary table of results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Label Smoothing':<20} {'Test Loss':<15} {'Test Acc (%)':<15} {'Last Token Acc (%)':<20}\")\n",
        "    print(\"-\"*80)\n",
        "    for ls_val in results.keys():\n",
        "        data = results[ls_val]\n",
        "        print(f\"{ls_val:<20.2f} {data['loss_test'][-1]:<15.4f} \"\n",
        "              f\"{data['acc_test'][-1]*100:<15.2f} {data['acc_last_test'][-1]*100:<20.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b379d7c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_smoothing_values = [0.0, 0.05, 0.1, 0.2]\n",
        "\n",
        "results = train_with_label_smoothing(\n",
        "    model=model,\n",
        "    params_init=params,  # Your initial parameters\n",
        "    opt_state_init=tx.init(params),  # Initial optimizer state\n",
        "    tx=tx,  # Your optimizer\n",
        "    train_text_int=train_text_int,\n",
        "    test_text_int=test_text_int,\n",
        "    train_step_fn=train_step_with_smoothing,\n",
        "    get_batch=get_batch,\n",
        "    label_smoothing_values=label_smoothing_values,\n",
        "    niter=5000,\n",
        "    B=128,\n",
        "    T=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136efef2",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig1 = plot_label_smoothing_comparison(results)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig2 = plot_final_comparison(results)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print_summary(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb70cd1d",
      "metadata": {},
      "source": [
        "### Tempeature scaling\n",
        "\n",
        "- temperature = 1.0 - baseline (no scaling)\n",
        "- temperature = 0.8 - sharper, more confident predictions\n",
        "- temperature = 1.2 - softer, less confident predictions\n",
        "- temperature = 0.5 - very sharp (risk of overconfidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe7c939",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove @jax.jit or add static_argnames\n",
        "from functools import partial\n",
        "\n",
        "@partial(jax.jit, static_argnames=['temperature'])\n",
        "def loss_and_metrics(logits, targets, temperature=1.0):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "\n",
        "    Args:\n",
        "      logits: (B, T, V) float array of unnormalized scores.\n",
        "      targets: (B, T) integer array with ground-truth class ids.\n",
        "      temperature: Float > 0. Scales logits before softmax.\n",
        "                   < 1.0 = sharper predictions (more confident)\n",
        "                   > 1.0 = softer predictions (less confident)\n",
        "\n",
        "    Returns:\n",
        "      loss: scalar average cross-entropy.\n",
        "      metrics: dict with keys \"loss\", \"acc\", and \"acc_last\".\n",
        "    \"\"\"\n",
        "    vocab = logits.shape[-1]\n",
        "    \n",
        "    # Apply temperature scaling to logits\n",
        "    scaled_logits = logits / temperature\n",
        "    \n",
        "    flat_logits = scaled_logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Compute cross-entropy\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    loss = per_pos.mean()\n",
        "\n",
        "    # Predictions and accuracy (use scaled logits)\n",
        "    preds = jnp.argmax(scaled_logits, axis=-1)\n",
        "    is_match = preds == targets\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "    acc_last = jnp.mean(is_match[:, -1].astype(jnp.float32))\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b34ff8dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(params, opt_state, x, y, tx, temperature=1.0):\n",
        "    \"\"\"Single optimization step using optax optimizer.\n",
        "\n",
        "    Args:\n",
        "      params: pytree of model parameters.\n",
        "      opt_state: optax optimizer state corresponding to `params`.\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "      tx: optax.GradientTransformation (already initialized).\n",
        "      temperature: Float > 0 for temperature scaling.\n",
        "\n",
        "    Returns:\n",
        "      new_params: updated parameters after one gradient step.\n",
        "      new_opt_state: updated optimizer state.\n",
        "      metrics: dict of scalar metrics (loss, acc).\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply({\"params\": params}, x)\n",
        "        loss, metrics = loss_and_metrics(logits, y, temperature=temperature)\n",
        "        return loss, metrics\n",
        "\n",
        "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "\n",
        "    # optax update: compute parameter updates and new optimizer state\n",
        "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, metrics\n",
        "\n",
        "# jit: mark both tx and temperature as static\n",
        "train_step = jax.jit(train_step, static_argnames=(\"tx\", \"temperature\"))\n",
        "\n",
        "# create a batch from the training data\n",
        "def get_batch(text_int, B, T):\n",
        "    \"\"\"Create a random batch of data from text_int.\n",
        "\n",
        "    Args:\n",
        "      text_int: 1D array of token ids.\n",
        "      B: batch size (number of sequences).\n",
        "      T: sequence length (number of tokens per sequence).\n",
        "\n",
        "    Returns:\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "    \"\"\"\n",
        "    # choose random starting indices for each sequence in the batch\n",
        "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
        "    # inputs are text from i to i+T\n",
        "    x = np.stack([text_int[i:i+T] for i in ix])\n",
        "    # targets are text from i+1 to i+T+1\n",
        "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
        "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)\n",
        "\n",
        "# define optax optimizer\n",
        "learning_rate = 0.001\n",
        "# Create Adam optimizer (Optax)\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "# Initialize optimizer state for current params\n",
        "opt_state = tx.init(params)\n",
        "print(f\"Initialized optimizer: Adam lr={learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b112ffe6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Define temperature values to test\n",
        "temperature_values = [0.5, 0.8, 1.0, 1.2]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for temp_val in temperature_values:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with temperature = {temp_val}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Reset parameters for each run\n",
        "    params = params_init  # Your initial parameters\n",
        "    opt_state = tx.init(params)\n",
        "    \n",
        "    loss_history = []\n",
        "    time_history = []\n",
        "    loss_test_history = []\n",
        "    acc_test_history = []\n",
        "    acc_last_test_history = []\n",
        "    time_test_history = []\n",
        "    time_start = time.time()\n",
        "    \n",
        "    niter = 5000\n",
        "    B, T = 128, 32\n",
        "    \n",
        "    for it in range(niter):\n",
        "        batch = get_batch(train_text_int, B, T)\n",
        "        x, y = batch[0], batch[1]\n",
        "        \n",
        "        # Train with specific temperature\n",
        "        params, opt_state, metrics = train_step(params, opt_state, x, y, tx, temperature=temp_val)\n",
        "        \n",
        "        loss_history.append(float(metrics['loss']))\n",
        "        time_history.append(time.time() - time_start)\n",
        "        \n",
        "        if it % (niter // 50) == 0 or it == niter - 1:\n",
        "            time_since_start = time.time() - time_start\n",
        "            \n",
        "            # Test evaluation\n",
        "            B_test, T_test = 1024, 32\n",
        "            test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "            test_x, test_y = test_batch[0], test_batch[1]\n",
        "            test_logits = model.apply({\"params\": params}, test_x)\n",
        "            \n",
        "            # Use same temperature for test evaluation\n",
        "            test_loss, test_metrics = loss_and_metrics(test_logits, test_y, temperature=temp_val)\n",
        "            \n",
        "            loss_test_history.append(float(test_loss))\n",
        "            acc_test_history.append(float(test_metrics['acc']))\n",
        "            acc_last_test_history.append(float(test_metrics['acc_last']))\n",
        "            time_test_history.append(time_since_start)\n",
        "            \n",
        "            print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
        "            print(f\"\\t \\t loss(train :: test): {metrics['loss']:.4f} :: {test_loss:.4f}\")\n",
        "            print(f\"\\t \\t accuracy (train :: test): {100*metrics['acc']:.1f}% :: {100*test_metrics['acc']:.1f}%\")\n",
        "            print(f\"\\t \\t accuracy (last character) (train :: test): {100*metrics['acc_last']:.1f}% :: {100*test_metrics['acc_last']:.1f}%\")\n",
        "            print()\n",
        "    \n",
        "    results[temp_val] = {\n",
        "        'loss_train': loss_history,\n",
        "        'loss_test': loss_test_history,\n",
        "        'acc_test': acc_test_history,\n",
        "        'acc_last_test': acc_last_test_history,\n",
        "        'time_train': time_history,\n",
        "        'time_test': time_test_history\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6e29e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_temperature_comparison(results):\n",
        "    \"\"\"\n",
        "    Plot comparison of different temperature values.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary with training histories for each temperature\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Temperature Scaling Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    colors = ['#d62728', '#ff7f0e', '#1f77b4', '#2ca02c']\n",
        "    labels_map = {\n",
        "        0.5: 'Very Sharp (0.5)',\n",
        "        0.8: 'Sharp (0.8)',\n",
        "        1.0: 'Baseline (1.0)',\n",
        "        1.2: 'Soft (1.2)'\n",
        "    }\n",
        "    \n",
        "    # Plot 1: Training Loss over time\n",
        "    ax1 = axes[0, 0]\n",
        "    for idx, (temp_val, data) in enumerate(results.items()):\n",
        "        ax1.plot(data['time_train'], data['loss_train'], \n",
        "                label=labels_map.get(temp_val, f'{temp_val}'), linewidth=1.5, \n",
        "                color=colors[idx % len(colors)], alpha=0.7)\n",
        "    ax1.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
        "    ax1.set_title('Training Loss vs Time', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Test Loss\n",
        "    ax2 = axes[0, 1]\n",
        "    for idx, (temp_val, data) in enumerate(results.items()):\n",
        "        ax2.plot(data['time_test'], data['loss_test'], \n",
        "                label=labels_map.get(temp_val, f'{temp_val}'), linewidth=2, \n",
        "                marker='o', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax2.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax2.set_ylabel('Test Loss', fontsize=12)\n",
        "    ax2.set_title('Test Loss vs Time', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Test Accuracy\n",
        "    ax3 = axes[1, 0]\n",
        "    for idx, (temp_val, data) in enumerate(results.items()):\n",
        "        acc_percent = [a * 100 for a in data['acc_test']]\n",
        "        ax3.plot(data['time_test'], acc_percent, \n",
        "                label=labels_map.get(temp_val, f'{temp_val}'), linewidth=2, \n",
        "                marker='s', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax3.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax3.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    ax3.set_title('Overall Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax3.legend(fontsize=10)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Last Token Accuracy\n",
        "    ax4 = axes[1, 1]\n",
        "    for idx, (temp_val, data) in enumerate(results.items()):\n",
        "        acc_last_percent = [a * 100 for a in data['acc_last_test']]\n",
        "        ax4.plot(data['time_test'], acc_last_percent, \n",
        "                label=labels_map.get(temp_val, f'{temp_val}'), linewidth=2, \n",
        "                marker='^', markersize=4, color=colors[idx % len(colors)])\n",
        "    ax4.set_xlabel('Time (seconds)', fontsize=12)\n",
        "    ax4.set_ylabel('Last Token Accuracy (%)', fontsize=12)\n",
        "    ax4.set_title('Last Token Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax4.legend(fontsize=10)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_temperature_final_comparison(results):\n",
        "    \"\"\"\n",
        "    Create bar charts comparing final metrics for temperature scaling.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle('Final Metrics Comparison - Temperature Scaling', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    temp_vals = list(results.keys())\n",
        "    labels = [f'{v}' for v in temp_vals]\n",
        "    \n",
        "    # Extract final values\n",
        "    final_loss = [results[v]['loss_test'][-1] for v in temp_vals]\n",
        "    final_acc = [results[v]['acc_test'][-1] * 100 for v in temp_vals]\n",
        "    final_acc_last = [results[v]['acc_last_test'][-1] * 100 for v in temp_vals]\n",
        "    \n",
        "    colors = ['#d62728', '#ff7f0e', '#1f77b4', '#2ca02c']\n",
        "    \n",
        "    # Plot 1: Final Test Loss\n",
        "    ax1 = axes[0]\n",
        "    bars1 = ax1.bar(labels, final_loss, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax1.set_xlabel('Temperature', fontsize=12)\n",
        "    ax1.set_ylabel('Test Loss', fontsize=12)\n",
        "    ax1.set_title('Final Test Loss', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Plot 2: Final Test Accuracy\n",
        "    ax2 = axes[1]\n",
        "    bars2 = ax2.bar(labels, final_acc, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax2.set_xlabel('Temperature', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title('Final Test Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    ax2.set_ylim([0, 100])\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # Plot 3: Final Last Token Accuracy\n",
        "    ax3 = axes[2]\n",
        "    bars3 = ax3.bar(labels, final_acc_last, color=colors[:len(labels)], alpha=0.7)\n",
        "    ax3.set_xlabel('Temperature', fontsize=12)\n",
        "    ax3.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax3.set_title('Final Last Token Accuracy', fontsize=13, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "    ax3.set_ylim([0, 100])\n",
        "    for bar in bars3:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def print_temperature_summary(results):\n",
        "    \"\"\"Print summary table of temperature results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY - TEMPERATURE SCALING\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Temperature':<20} {'Test Loss':<15} {'Test Acc (%)':<15} {'Last Token Acc (%)':<20}\")\n",
        "    print(\"-\"*80)\n",
        "    for temp_val in results.keys():\n",
        "        data = results[temp_val]\n",
        "        print(f\"{temp_val:<20.1f} {data['loss_test'][-1]:<15.4f} \"\n",
        "              f\"{data['acc_test'][-1]*100:<15.2f} {data['acc_last_test'][-1]*100:<20.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0778e8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "fig1 = plot_temperature_comparison(results)\n",
        "plt.show()\n",
        "\n",
        "fig2 = plot_temperature_final_comparison(results)\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print_temperature_summary(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
