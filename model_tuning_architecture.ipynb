{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "LNS1ztqUUrw7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNS1ztqUUrw7",
        "outputId": "21ffc255-cbb8-406b-e302-508d14d5848f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'char-llm-assignment'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 12 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 30.14 MiB | 32.11 MiB/s, done.\n",
            "/content/char-llm-assignment\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/beckhamtoh/char-llm-assignment.git\n",
        "%cd char-llm-assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e2def0f4",
      "metadata": {
        "id": "e2def0f4"
      },
      "outputs": [],
      "source": [
        "# Enable autoreload of local Python modules (e.g., models)\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "# manual reload for local modules\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5001b05",
      "metadata": {
        "id": "d5001b05"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# local imports\n",
        "import models.models as models\n",
        "import util.generation as generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "906db4f2",
      "metadata": {
        "id": "906db4f2"
      },
      "outputs": [],
      "source": [
        "# initialize the jax random key\n",
        "key = jax.random.key(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762a1c8f",
      "metadata": {
        "id": "762a1c8f"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "86825275",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86825275",
        "outputId": "51b635db-0434-44d4-ccc9-aa0b1093834e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of training text: 90_000_000 characters\n",
            "Length of test text: 5_000_000 characters\n"
          ]
        }
      ],
      "source": [
        "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
        "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
        "    train_text = f.read()\n",
        "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
        "    test_text = f.read()\n",
        "\n",
        "# print the length of the training text and test text\n",
        "print(f\"Length of training text: {len(train_text):_} characters\")\n",
        "print(f\"Length of test text: {len(test_text):_} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfdca63",
      "metadata": {
        "id": "7bfdca63"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "34b1eafe",
      "metadata": {
        "id": "34b1eafe"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary (lowercase + space + a few punctuations)\n",
        "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
        "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
        "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
        "\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to array of integers\"\"\"\n",
        "    ids = [char_to_int[c] for c in s]\n",
        "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "42b5af70",
      "metadata": {
        "id": "42b5af70"
      },
      "outputs": [],
      "source": [
        "# encode the text\n",
        "train_text_int = encode(train_text)\n",
        "test_text_int = encode(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6458536d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6458536d",
        "outputId": "f7572de8-c91b-4039-cedd-c6aab21535c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "of the illness and went to the fragrant mountain to give thanks to the person when he discovered that his own daughter gave up h\n",
            "\n",
            "together they performed the arcade fire s song wake up from their album funeral he joined them again on one five september singi\n",
            "\n",
            "ro asiatic language phylum its closest relatives are the berber semitic and beja groups of languages written records of the egyp\n",
            "\n",
            "es were produced between about one nine three zero and one nine three five but the concept was abandonded because of its limited\n",
            "\n",
            "xt is read aloud twice during the celebration setting the biblical book of esther is set in the third year of ahasuerus a king o\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# sanity check: display a few random characters from the training text\n",
        "T = 128\n",
        "for _ in range(5):\n",
        "    # choose random position in text\n",
        "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
        "    print(train_text[N:N+T])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7724c34b",
      "metadata": {
        "id": "7724c34b"
      },
      "source": [
        "# Create a basic Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f5955c3f",
      "metadata": {
        "id": "f5955c3f"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
        "    # create a basic Transformer model\n",
        "    model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
        "    # create a dummy input for initialization\n",
        "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
        "    # pass the dummy input to the model to initialize the parameters\n",
        "    params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
        "    return model, params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1837688",
      "metadata": {},
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b40883a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to count parameters\n",
        "def count_params(params):\n",
        "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "\n",
        "# Loss function\n",
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "    Assumes targets contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
        "    Args:\n",
        "        logits: (B, T, V) float array of unnormalized scores.\n",
        "        targets: (B, T) integer array with ground-truth class ids.\n",
        "    Returns:\n",
        "        loss: scalar average cross-entropy over all positions.\n",
        "        metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
        "    \"\"\"\n",
        "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "    # Per-position cross-entropy, then mean over all positions\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    loss = per_pos.mean()\n",
        "    # prediction over all positions\n",
        "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
        "    # compute accuracy over only the last position\n",
        "    is_match = preds == targets\n",
        "    # Accuracy over all positions\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "    # Accuracy over only last position\n",
        "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:, -1])\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}\n",
        "\n",
        "\n",
        "# Batch creation function\n",
        "def get_batch(text_int, B, T):\n",
        "    \"\"\"Create a random batch of data from text_int.\n",
        "    Args:\n",
        "        text_int: 1D array of token ids.\n",
        "        B: batch size (number of sequences).\n",
        "        T: sequence length (number of tokens per sequence).\n",
        "    Returns:\n",
        "        x: (B, T) int array input tokens.\n",
        "        y: (B, T) int array target tokens.\n",
        "    \"\"\"\n",
        "    # choose random starting indices for each sequence in the batch\n",
        "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
        "    # inputs are text from i to i+T\n",
        "    x = np.stack([text_int[i:i+T] for i in ix])\n",
        "    # targets are text from i+1 to i+T+1\n",
        "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
        "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)\n",
        "\n",
        "\n",
        "# Optimization step\n",
        "def train_step(params, opt_state, x, y, tx):\n",
        "    \"\"\"Single optimization step using optax optimizer.\n",
        "    Args:\n",
        "        params: pytree of model parameters.\n",
        "        opt_state: optax optimizer state corresponding to params.\n",
        "        x: (B, T) int array input tokens.\n",
        "        y: (B, T) int array target tokens.\n",
        "        tx: optax.GradientTransformation (already initialized).\n",
        "    Returns:\n",
        "        new_params: updated parameters after one gradient step.\n",
        "        new_opt_state: updated optimizer state.\n",
        "        metrics: dict of scalar metrics (loss, acc).\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply({\"params\": params}, x)\n",
        "        loss, metrics = loss_and_metrics(logits, y)\n",
        "        return loss, metrics\n",
        "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "    # optax update: compute parameter updates and new optimizer state\n",
        "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, metrics\n",
        "\n",
        "# jit the train_step function (but keep model reference available)\n",
        "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aadd358",
      "metadata": {},
      "source": [
        "# Hyperparameter Search Framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ae6069",
      "metadata": {},
      "source": [
        "# Experiment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3588d3a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STAGE 1: Quick exploration (1k iterations each)\n",
        "# Goal: Identify which regions of hyperparameter space are promising\n",
        "stage1_configs = [\n",
        "    {\"d_model\": 128, \"n_layers\": 2, \"n_heads\": 2, \"name\": \"s1_tiny\", \"niter\": 1000},\n",
        "    {\"d_model\": 128, \"n_layers\": 4, \"n_heads\": 4, \"name\": \"s1_small_deep\", \"niter\": 1000},\n",
        "    {\"d_model\": 256, \"n_layers\": 2, \"n_heads\": 4, \"name\": \"s1_medium_shallow\", \"niter\": 1000},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 4, \"name\": \"s1_medium_medium\", \"niter\": 1000},\n",
        "    {\"d_model\": 512, \"n_layers\": 2, \"n_heads\": 8, \"name\": \"s1_large_shallow\", \"niter\": 1000},\n",
        "    {\"d_model\": 512, \"n_layers\": 4, \"n_heads\": 8, \"name\": \"s1_large_medium\", \"niter\": 1000},\n",
        "]\n",
        "\n",
        "# STAGE 2: Ablation studies (1k iterations each)\n",
        "# Goal: Understand impact of each hyperparameter while keeping others constant\n",
        "stage2_configs = [\n",
        "    # Ablation 1: Varying depth (d_model=256, n_heads=4, head_dim=64)\n",
        "    {\"d_model\": 256, \"n_layers\": 1, \"n_heads\": 4, \"name\": \"s2_depth_1L\", \"niter\": 1000, \"group\": \"depth\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 2, \"n_heads\": 4, \"name\": \"s2_depth_2L\", \"niter\": 1000, \"group\": \"depth\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 4, \"name\": \"s2_depth_4L\", \"niter\": 1000, \"group\": \"depth\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 6, \"n_heads\": 4, \"name\": \"s2_depth_6L\", \"niter\": 1000, \"group\": \"depth\"},\n",
        "    \n",
        "    # Ablation 2: Varying hidden size (n_layers=4, head_dim≈64)\n",
        "    {\"d_model\": 128, \"n_layers\": 4, \"n_heads\": 2, \"name\": \"s2_width_128\", \"niter\": 1000, \"group\": \"width\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 4, \"name\": \"s2_width_256\", \"niter\": 1000, \"group\": \"width\"},\n",
        "    {\"d_model\": 512, \"n_layers\": 4, \"n_heads\": 8, \"name\": \"s2_width_512\", \"niter\": 1000, \"group\": \"width\"},\n",
        "    \n",
        "    # Ablation 3: Varying head_dim (d_model=256, n_layers=4)\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 16, \"name\": \"s2_head16\", \"niter\": 1000, \"group\": \"head_dim\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 8, \"name\": \"s2_head32\", \"niter\": 1000, \"group\": \"head_dim\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 4, \"name\": \"s2_head64\", \"niter\": 1000, \"group\": \"head_dim\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 2, \"name\": \"s2_head128\", \"niter\": 1000, \"group\": \"head_dim\"},\n",
        "]\n",
        "\n",
        "# STAGE 3: Interaction exploration (1k iterations each)\n",
        "# Goal: Test how hyperparameters interact (wide vs deep, etc.)\n",
        "stage3_configs = [\n",
        "    # Wide & shallow vs narrow & deep trade-offs\n",
        "    {\"d_model\": 512, \"n_layers\": 2, \"n_heads\": 8, \"name\": \"s3_wide_shallow\", \"niter\": 1000, \"group\": \"tradeoff\"},\n",
        "    {\"d_model\": 256, \"n_layers\": 6, \"n_heads\": 4, \"name\": \"s3_narrow_deep\", \"niter\": 1000, \"group\": \"tradeoff\"},\n",
        "    {\"d_model\": 384, \"n_layers\": 4, \"n_heads\": 6, \"name\": \"s3_balanced\", \"niter\": 1000, \"group\": \"tradeoff\"},\n",
        "    \n",
        "    # Test with head_dim=32 (character-level might prefer smaller)\n",
        "    {\"d_model\": 256, \"n_layers\": 4, \"n_heads\": 8, \"name\": \"s3_head32_medium\", \"niter\": 1000, \"group\": \"char_level\"},\n",
        "    {\"d_model\": 512, \"n_layers\": 6, \"n_heads\": 16, \"name\": \"s3_head32_large\", \"niter\": 1000, \"group\": \"char_level\"},\n",
        "]\n",
        "\n",
        "# STAGE 4: Best candidates (will fill in after seeing results)\n",
        "# Train top 2-3 models to convergence\n",
        "stage4_configs = [\n",
        "    # UNCOMMENT AND FILL IN AFTER RUNNING STAGES 1-3\n",
        "    # {\"d_model\": ???, \"n_layers\": ???, \"n_heads\": ???, \"name\": \"s4_best_1\", \"niter\": 150000},\n",
        "    # {\"d_model\": ???, \"n_layers\": ???, \"n_heads\": ???, \"name\": \"s4_best_2\", \"niter\": 150000},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d362f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "configurations = stage1_configs + stage2_configs + stage3_configs + stage4_configs\n",
        "\n",
        "# Add computed fields\n",
        "for config in configurations:\n",
        "    config['head_dim'] = config['d_model'] // config['n_heads']\n",
        "    config['group'] = config.get('group', 'exploration')\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"EXPERIMENTAL SETUP\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total configurations to test: {len(configurations)}\")\n",
        "print(f\"\\nConfiguration summary:\")\n",
        "print(pd.DataFrame(configurations)[['name', 'd_model', 'n_layers', 'n_heads', 'head_dim', 'niter', 'group']])\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6126e325",
      "metadata": {},
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a18efb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_with_tracking(config, train_text_int, test_text_int, vocab_size, max_len, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a single model configuration and track performance over time.\n",
        "    \n",
        "    Returns:\n",
        "        result: dict with final metrics and training history\n",
        "        params: trained model parameters\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training: {config['name']}\")\n",
        "    print(f\"  Architecture: d_model={config['d_model']}, n_layers={config['n_layers']}, \"\n",
        "          f\"n_heads={config['n_heads']}, head_dim={config['head_dim']}\")\n",
        "    print(f\"  Training: {config['niter']:,} iterations\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create model\n",
        "    model, params = create_train_state(\n",
        "        key, \n",
        "        vocab_size=vocab_size,\n",
        "        d_model=config['d_model'],\n",
        "        n_layers=config['n_layers'],\n",
        "        n_heads=config['n_heads'],\n",
        "        max_len=max_len\n",
        "    )\n",
        "    \n",
        "    n_params = count_params(params)\n",
        "    print(f\"  Parameters: {n_params:,}\")\n",
        "    \n",
        "    # Setup optimizer\n",
        "    tx = optax.adam(learning_rate=learning_rate)\n",
        "    opt_state = tx.init(params)\n",
        "    \n",
        "    # Training configuration\n",
        "    niter = config['niter']\n",
        "    B, T = 128, 32\n",
        "    B_test, T_test = 1024, 32\n",
        "    \n",
        "    # Tracking\n",
        "    time_start = time.time()\n",
        "    checkpoint_interval = max(1, niter // 20)  # 20 checkpoints throughout training\n",
        "    \n",
        "    history = {\n",
        "        'iteration': [],\n",
        "        'time_elapsed': [],\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'test_acc_last': []\n",
        "    }\n",
        "    \n",
        "    # Training loop\n",
        "    for it in range(niter):\n",
        "        # Training step\n",
        "        batch = get_batch(train_text_int, B, T)\n",
        "        input, target = batch[0], batch[1]\n",
        "        params, opt_state, metrics = train_step(params, opt_state, input, target, tx)\n",
        "        \n",
        "        # Periodic evaluation\n",
        "        if it % checkpoint_interval == 0 or it == niter - 1:\n",
        "            time_elapsed = time.time() - time_start\n",
        "            \n",
        "            # Test evaluation\n",
        "            test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "            test_input, test_target = test_batch[0], test_batch[1]\n",
        "            test_logits = model.apply({\"params\": params}, test_input)\n",
        "            test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
        "            \n",
        "            # Record history\n",
        "            history['iteration'].append(it)\n",
        "            history['time_elapsed'].append(time_elapsed)\n",
        "            history['train_loss'].append(float(metrics['loss']))\n",
        "            history['test_loss'].append(float(test_loss))\n",
        "            history['test_acc'].append(float(test_metrics['acc']))\n",
        "            history['test_acc_last'].append(float(test_metrics['acc_last']))\n",
        "            \n",
        "            # Print progress\n",
        "            print(f\"  iter {it:>7,} | time {time_elapsed:>7.1f}s | \"\n",
        "                  f\"train_loss {metrics['loss']:.4f} | test_loss {test_loss:.4f} | \"\n",
        "                  f\"test_acc {100*test_metrics['acc']:.2f}% | \"\n",
        "                  f\"test_acc_last {100*test_metrics['acc_last']:.2f}%\")\n",
        "    \n",
        "    total_time = time.time() - time_start\n",
        "    \n",
        "    # Final evaluation (one more time on test set)\n",
        "    test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "    test_input, test_target = test_batch[0], test_batch[1]\n",
        "    test_logits = model.apply({\"params\": params}, test_input)\n",
        "    final_test_loss, final_test_metrics = loss_and_metrics(test_logits, test_target)\n",
        "    \n",
        "    # Compile results\n",
        "    result = {\n",
        "        'name': config['name'],\n",
        "        'd_model': config['d_model'],\n",
        "        'n_layers': config['n_layers'],\n",
        "        'n_heads': config['n_heads'],\n",
        "        'head_dim': config['head_dim'],\n",
        "        'n_params': n_params,\n",
        "        'niter': niter,\n",
        "        'total_time': total_time,\n",
        "        'time_per_iter': total_time / niter,\n",
        "        'iters_per_sec': niter / total_time,\n",
        "        'final_train_loss': float(metrics['loss']),\n",
        "        'final_test_loss': float(final_test_loss),\n",
        "        'final_test_acc': float(final_test_metrics['acc']),\n",
        "        'final_test_acc_last': float(final_test_metrics['acc_last']),\n",
        "        'group': config['group'],\n",
        "        'history': history\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"  FINAL RESULTS:\")\n",
        "    print(f\"  Test Loss: {final_test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy (all positions): {100*final_test_metrics['acc']:.2f}%\")\n",
        "    print(f\"  Test Accuracy (last char): {100*final_test_metrics['acc_last']:.2f}%\")\n",
        "    print(f\"  Training Time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
        "    print(f\"  Speed: {niter/total_time:.1f} iter/s\")\n",
        "    print(f\"{'='*40}\\n\")\n",
        "    \n",
        "    return result, params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096eb51e",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667ef8c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global settings\n",
        "vocab_size = len(char_set)\n",
        "max_len = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "results = []\n",
        "trained_models = {}\n",
        "\n",
        "print(f\"\\n{'#'*80}\")\n",
        "print(f\"STARTING HYPERPARAMETER SEARCH\")\n",
        "print(f\"{'#'*80}\\n\")\n",
        "\n",
        "experiment_start_time = time.time()\n",
        "\n",
        "for config_idx, config in enumerate(configurations):\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"Configuration {config_idx + 1}/{len(configurations)}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    \n",
        "    # Train model\n",
        "    result, params = train_model_with_tracking(\n",
        "        config, \n",
        "        train_text_int, \n",
        "        test_text_int, \n",
        "        vocab_size, \n",
        "        max_len, \n",
        "        learning_rate\n",
        "    )\n",
        "    \n",
        "    results.append(result)\n",
        "    trained_models[config['name']] = params\n",
        "    \n",
        "    # Save intermediate results (in case of crash)\n",
        "    temp_df = pd.DataFrame(results)\n",
        "    temp_df.to_csv('experiment_results_temp.csv', index=False)\n",
        "\n",
        "total_experiment_time = time.time() - experiment_start_time\n",
        "\n",
        "print(f\"\\n{'#'*80}\")\n",
        "print(f\"ALL EXPERIMENTS COMPLETE\")\n",
        "print(f\"Total time: {total_experiment_time/60:.1f} minutes ({total_experiment_time/3600:.2f} hours)\")\n",
        "print(f\"{'#'*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5551d885",
      "metadata": {},
      "source": [
        "# Download results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b1d024",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('experiment_results_final.csv', index=False)\n",
        "print(f\"✓ Results saved to 'experiment_results_final.csv'\")\n",
        "\n",
        "# Save training histories separately (they're large)\n",
        "histories = {r['name']: r['history'] for r in results}\n",
        "with open('training_histories.json', 'w') as f:\n",
        "    json.dump(histories, f)\n",
        "print(f\"✓ Training histories saved to 'training_histories.json'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e63748",
      "metadata": {},
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e634ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"SUMMARY OF ALL CONFIGURATIONS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "summary_cols = ['name', 'd_model', 'n_layers', 'n_heads', 'head_dim', 'n_params', \n",
        "                'final_test_acc_last', 'final_test_loss', 'total_time', 'group']\n",
        "print(results_df[summary_cols].to_string(index=False))\n",
        "\n",
        "# Rank by test accuracy\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"TOP 10 MODELS BY TEST ACCURACY (LAST CHARACTER)\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "results_df_sorted = results_df.sort_values('final_test_acc_last', ascending=False)\n",
        "top10_cols = ['name', 'n_params', 'final_test_acc_last', 'final_test_loss', 'total_time']\n",
        "print(results_df_sorted[top10_cols].head(10).to_string(index=False))\n",
        "\n",
        "# Best in each group\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"BEST MODEL IN EACH EXPERIMENTAL GROUP\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for group in results_df['group'].unique():\n",
        "    group_df = results_df[results_df['group'] == group]\n",
        "    best_in_group = group_df.sort_values('final_test_acc_last', ascending=False).iloc[0]\n",
        "    print(f\"{group:15s} : {best_in_group['name']:20s} \"\n",
        "          f\"acc={100*best_in_group['final_test_acc_last']:.2f}% \"\n",
        "          f\"params={best_in_group['n_params']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416135da",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"GENERATING PLOTS...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Model size vs accuracy\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "scatter = ax1.scatter(results_df['n_params'], results_df['final_test_acc_last'] * 100, \n",
        "                      c=results_df['total_time'], s=100, alpha=0.6, cmap='viridis')\n",
        "ax1.set_xlabel('Number of Parameters')\n",
        "ax1.set_ylabel('Test Accuracy (Last Char) %')\n",
        "ax1.set_title('Model Size vs Performance')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=ax1, label='Training Time (s)')\n",
        "\n",
        "# Plot 2: Training time vs accuracy\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.scatter(results_df['total_time'], results_df['final_test_acc_last'] * 100, \n",
        "            s=100, alpha=0.6, color='orange')\n",
        "ax2.set_xlabel('Training Time (seconds)')\n",
        "ax2.set_ylabel('Test Accuracy (Last Char) %')\n",
        "ax2.set_title('Training Time vs Performance')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Efficiency (accuracy per hour)\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "results_df['acc_per_hour'] = results_df['final_test_acc_last'] * 100 / (results_df['total_time'] / 3600)\n",
        "top_efficient = results_df.nlargest(10, 'acc_per_hour')\n",
        "ax3.barh(range(len(top_efficient)), top_efficient['acc_per_hour'])\n",
        "ax3.set_yticks(range(len(top_efficient)))\n",
        "ax3.set_yticklabels(top_efficient['name'], fontsize=8)\n",
        "ax3.set_xlabel('Accuracy Gain per Hour (%/hr)')\n",
        "ax3.set_title('Top 10 Most Efficient Models')\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 4: Impact of depth (ablation group)\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "depth_results = results_df[results_df['group'] == 'depth'].sort_values('n_layers')\n",
        "if len(depth_results) > 0:\n",
        "    ax4.plot(depth_results['n_layers'], depth_results['final_test_acc_last'] * 100, \n",
        "             marker='o', linewidth=2, markersize=8)\n",
        "    ax4.set_xlabel('Number of Layers')\n",
        "    ax4.set_ylabel('Test Accuracy (Last Char) %')\n",
        "    ax4.set_title('Ablation: Impact of Model Depth')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Impact of width (ablation group)\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "width_results = results_df[results_df['group'] == 'width'].sort_values('d_model')\n",
        "if len(width_results) > 0:\n",
        "    ax5.plot(width_results['d_model'], width_results['final_test_acc_last'] * 100, \n",
        "             marker='s', linewidth=2, markersize=8, color='green')\n",
        "    ax5.set_xlabel('Hidden Size (d_model)')\n",
        "    ax5.set_ylabel('Test Accuracy (Last Char) %')\n",
        "    ax5.set_title('Ablation: Impact of Hidden Size')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Impact of head_dim (ablation group)\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "head_results = results_df[results_df['group'] == 'head_dim'].sort_values('head_dim')\n",
        "if len(head_results) > 0:\n",
        "    ax6.plot(head_results['head_dim'], head_results['final_test_acc_last'] * 100, \n",
        "             marker='^', linewidth=2, markersize=8, color='red')\n",
        "    ax6.set_xlabel('Head Dimension (d_model / n_heads)')\n",
        "    ax6.set_ylabel('Test Accuracy (Last Char) %')\n",
        "    ax6.set_title('Ablation: Impact of Head Dimension')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Training curves (top 5 models)\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "top5 = results_df.nlargest(5, 'final_test_acc_last')\n",
        "for idx, row in top5.iterrows():\n",
        "    history = row['history']\n",
        "    ax7.plot(history['time_elapsed'], \n",
        "             [x*100 for x in history['test_acc_last']], \n",
        "             label=f\"{row['name']} ({row['n_params']:,} params)\", \n",
        "             linewidth=2)\n",
        "ax7.set_xlabel('Time (seconds)')\n",
        "ax7.set_ylabel('Test Accuracy (Last Char) %')\n",
        "ax7.set_title('Learning Curves: Top 5 Models')\n",
        "ax7.legend(loc='lower right')\n",
        "ax7.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig('hyperparameter_search_results.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Main plot saved to 'hyperparameter_search_results.png'\")\n",
        "plt.show()\n",
        "\n",
        "# Additional plot: Params vs Time vs Accuracy (3D-like)\n",
        "fig2, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "scatter = ax.scatter(results_df['n_params'], \n",
        "                     results_df['total_time'], \n",
        "                     s=results_df['final_test_acc_last']*5000,  # size = accuracy\n",
        "                     c=results_df['final_test_acc_last']*100,\n",
        "                     cmap='RdYlGn', \n",
        "                     alpha=0.6,\n",
        "                     edgecolors='black',\n",
        "                     linewidth=0.5)\n",
        "ax.set_xlabel('Number of Parameters', fontsize=12)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "ax.set_title('Model Complexity vs Training Cost\\n(bubble size and color = accuracy)', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Test Accuracy (%)', fontsize=11)\n",
        "\n",
        "# Annotate best models\n",
        "top3 = results_df.nlargest(3, 'final_test_acc_last')\n",
        "for idx, row in top3.iterrows():\n",
        "    ax.annotate(row['name'], \n",
        "                (row['n_params'], row['total_time']),\n",
        "                xytext=(10, 10), \n",
        "                textcoords='offset points',\n",
        "                fontsize=9,\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_complexity_vs_cost.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Complexity plot saved to 'model_complexity_vs_cost.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4dbf366",
      "metadata": {},
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ec84a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"CREATING DOWNLOADABLE PACKAGE...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Create a timestamp for the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f'architecture_search_results_{timestamp}.zip'\n",
        "\n",
        "# List of files to include in the zip\n",
        "files_to_zip = [\n",
        "    'experiment_results_final.csv',\n",
        "    'training_histories.json',\n",
        "    'hyperparameter_search_results.png',\n",
        "    'model_complexity_vs_cost.png',\n",
        "]\n",
        "\n",
        "# Create a summary text file\n",
        "summary_filename = 'experiment_summary.txt'\n",
        "with open(summary_filename, 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"ARCHITECTURE SEARCH EXPERIMENT SUMMARY\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Total configurations tested: {len(results_df)}\\n\")\n",
        "    f.write(f\"Total experiment time: {total_experiment_time/60:.1f} minutes\\n\\n\")\n",
        "    \n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"TOP 10 MODELS BY TEST ACCURACY (LAST CHARACTER)\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(results_df_sorted[top10_cols].head(10).to_string(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "    \n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"BEST MODEL IN EACH EXPERIMENTAL GROUP\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    for group in results_df['group'].unique():\n",
        "        group_df = results_df[results_df['group'] == group]\n",
        "        best_in_group = group_df.sort_values('final_test_acc_last', ascending=False).iloc[0]\n",
        "        f.write(f\"{group:15s} : {best_in_group['name']:20s} \")\n",
        "        f.write(f\"acc={100*best_in_group['final_test_acc_last']:.2f}% \")\n",
        "        f.write(f\"params={best_in_group['n_params']:,}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"ALL CONFIGURATIONS SUMMARY\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(results_df[summary_cols].to_string(index=False))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "files_to_zip.append(summary_filename)\n",
        "\n",
        "# Create the zip file\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for file in files_to_zip:\n",
        "        if os.path.exists(file):\n",
        "            zipf.write(file, arcname=file)\n",
        "            print(f\"  ✓ Added: {file}\")\n",
        "        else:\n",
        "            print(f\"  ✗ Missing: {file}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"PACKAGE CREATED SUCCESSFULLY!\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(f\"Zip file: {zip_filename}\")\n",
        "print(f\"Size: {os.path.getsize(zip_filename) / 1024:.2f} KB\")\n",
        "\n",
        "# ============================================================================\n",
        "# DISPLAY DOWNLOAD LINKS (KAGGLE SPECIFIC)\n",
        "# ============================================================================\n",
        "\n",
        "from IPython.display import FileLink, display, HTML\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"DOWNLOAD YOUR RESULTS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Create download links\n",
        "print(\"📦 Download the complete package:\")\n",
        "display(FileLink(zip_filename))\n",
        "\n",
        "print(\"\\n📄 Or download individual files:\")\n",
        "for file in files_to_zip:\n",
        "    if os.path.exists(file):\n",
        "        display(FileLink(file))\n",
        "\n",
        "# Also create an HTML summary for easier viewing in Kaggle\n",
        "html_summary = f\"\"\"\n",
        "<div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
        "    <h2 style=\"color: #2c3e50;\">🎯 Experiment Complete!</h2>\n",
        "    <h3>Download Options:</h3>\n",
        "    <ul style=\"font-size: 14px;\">\n",
        "        <li><strong>Complete Package:</strong> <code>{zip_filename}</code> - Contains all results, plots, and summaries</li>\n",
        "        <li><strong>CSV Results:</strong> <code>experiment_results_final.csv</code> - For analysis in Excel/Python</li>\n",
        "        <li><strong>Training Histories:</strong> <code>training_histories.json</code> - Detailed training curves</li>\n",
        "        <li><strong>Plots:</strong> PNG files for your report</li>\n",
        "        <li><strong>Summary:</strong> <code>experiment_summary.txt</code> - Text summary of all results</li>\n",
        "    </ul>\n",
        "    <h3>Top 3 Models:</h3>\n",
        "    <ol>\n",
        "\"\"\"\n",
        "\n",
        "for idx, (_, row) in enumerate(results_df_sorted.head(3).iterrows(), 1):\n",
        "    html_summary += f\"\"\"\n",
        "        <li><strong>{row['name']}</strong>: \n",
        "            {100*row['final_test_acc_last']:.2f}% accuracy \n",
        "            ({row['n_params']:,} params, {row['total_time']:.0f}s training)\n",
        "        </li>\n",
        "    \"\"\"\n",
        "\n",
        "html_summary += \"\"\"\n",
        "    </ol>\n",
        "    <p style=\"margin-top: 20px; font-style: italic;\">\n",
        "        💡 Click the links above to download files directly from Kaggle!\n",
        "    </p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_summary))\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"EXPERIMENT COMPLETE!\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(f\"Next steps:\")\n",
        "print(f\"1. Download the zip file or individual files using the links above\")\n",
        "print(f\"2. Review the plots and tables\")\n",
        "print(f\"3. Identify the top 2-3 architectures\")\n",
        "print(f\"4. Add them to stage4_configs and train for 150k iterations\")\n",
        "print(f\"5. Use the best model for your final report\")\n",
        "print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# KAGGLE OUTPUT PATH (ALTERNATIVE METHOD)\n",
        "# ============================================================================\n",
        "# If the FileLink method doesn't work, files are also available at:\n",
        "print(\"📁 Files are also saved in the working directory:\")\n",
        "print(f\"   {os.getcwd()}\")\n",
        "print(\"\\n   You can find them in the Kaggle output section after the notebook runs.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
